#!/usr/bin/env python3
"""
Comprehensive BigQuery Setup for ustam App
Creates all tables, views, and analytics infrastructure
"""

import os
import sys
import json
import subprocess
import logging
from datetime import datetime, timedelta
from google.cloud import bigquery
from google.api_core import exceptions

# Add the backend directory to the path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ComprehensiveBigQuerySetup:
    """Comprehensive BigQuery setup for ustam app with full analytics infrastructure"""
    
    def __init__(self, project_id=None):
        self.project_id = project_id or "ustaapp-analytics"
        self.dataset_id = "ustam_analytics"
        self.location = "US"
        self.client = None
        
        # All table schemas
        self.table_schemas = {
            # Core tables (existing)
            'users': 'schemas/users_schema.json',
            'categories': 'schemas/categories_schema.json',
            'customers': 'schemas/customers_schema.json',
            'craftsmen': 'schemas/craftsmen_schema.json',
            'jobs': 'schemas/jobs_schema.json',
            'messages': 'schemas/messages_schema.json',
            'notifications': 'schemas/notifications_schema.json',
            'payments': 'schemas/payments_schema.json',
            'quotes': 'schemas/quotes_schema.json',
            'reviews': 'schemas/reviews_schema.json',
            
            # New analytics tables
            'user_activity_logs': 'schemas/user_activity_logs_schema.json',
            'business_metrics': 'schemas/business_metrics_schema.json',
            'error_logs': 'schemas/error_logs_schema.json',
            'performance_metrics': 'schemas/performance_metrics_schema.json',
            'search_analytics': 'schemas/search_analytics_schema.json',
            'payment_analytics': 'schemas/payment_analytics_schema.json',
        }
        
        # Analytics views to create
        self.analytics_views = {
            'daily_user_stats': self._get_daily_user_stats_query(),
            'craftsman_performance': self._get_craftsman_performance_query(),
            'revenue_dashboard': self._get_revenue_dashboard_query(),
            'search_insights': self._get_search_insights_query(),
            'error_summary': self._get_error_summary_query(),
            'platform_comparison': self._get_platform_comparison_query(),
            'business_kpis': self._get_business_kpis_query(),
        }

    def initialize_client(self):
        """Initialize BigQuery client"""
        try:
            self.client = bigquery.Client(project=self.project_id)
            logger.info(f"âœ… BigQuery client initialized for project: {self.project_id}")
            return True
        except Exception as e:
            logger.error(f"âŒ Failed to initialize BigQuery client: {e}")
            return False

    def check_prerequisites(self):
        """Check if all prerequisites are met"""
        logger.info("ğŸ” Checking prerequisites...")
        
        # Check gcloud authentication
        try:
            result = subprocess.run(['gcloud', 'auth', 'list'], capture_output=True, text=True)
            if 'ACTIVE' not in result.stdout:
                logger.error("âŒ No active gcloud authentication found. Run: gcloud auth login")
                return False
            logger.info("âœ… Google Cloud authentication verified")
        except FileNotFoundError:
            logger.error("âŒ gcloud command not found. Install Google Cloud SDK")
            return False
        
        # Check BigQuery API
        try:
            result = subprocess.run(['gcloud', 'services', 'list', '--enabled', '--filter=bigquery'], 
                                  capture_output=True, text=True)
            if 'bigquery' not in result.stdout:
                logger.error("âŒ BigQuery API not enabled. Run: gcloud services enable bigquery.googleapis.com")
                return False
            logger.info("âœ… BigQuery API is enabled")
        except Exception as e:
            logger.warning(f"âš ï¸ Could not verify BigQuery API status: {e}")
        
        return True

    def create_dataset(self):
        """Create BigQuery dataset if it doesn't exist"""
        try:
            dataset_ref = self.client.dataset(self.dataset_id)
            
            try:
                dataset = self.client.get_dataset(dataset_ref)
                logger.info(f"âœ… Dataset {self.dataset_id} already exists")
            except exceptions.NotFound:
                # Create dataset
                dataset = bigquery.Dataset(dataset_ref)
                dataset.location = self.location
                dataset.description = "ustam App Analytics and Logging Data"
                
                # Set dataset labels
                dataset.labels = {
                    "app": "ustam",
                    "environment": "production",
                    "created_by": "auto_setup"
                }
                
                dataset = self.client.create_dataset(dataset)
                logger.info(f"âœ… Dataset {self.dataset_id} created successfully")
            
            return True
        except Exception as e:
            logger.error(f"âŒ Failed to create dataset: {e}")
            return False

    def load_schema(self, schema_file):
        """Load table schema from JSON file"""
        try:
            schema_path = os.path.join('bigquery_exports', schema_file)
            with open(schema_path, 'r', encoding='utf-8') as f:
                schema_data = json.load(f)
            
            schema = []
            for field in schema_data:
                schema.append(bigquery.SchemaField(
                    name=field['name'],
                    field_type=field['type'],
                    mode=field['mode'],
                    description=field.get('description', '')
                ))
            
            return schema
        except Exception as e:
            logger.error(f"âŒ Failed to load schema from {schema_file}: {e}")
            return None

    def create_table(self, table_name, schema_file):
        """Create a BigQuery table with given schema"""
        try:
            table_ref = self.client.dataset(self.dataset_id).table(table_name)
            
            # Check if table exists
            try:
                table = self.client.get_table(table_ref)
                logger.info(f"âœ… Table {table_name} already exists")
                return True
            except exceptions.NotFound:
                pass
            
            # Load schema
            schema = self.load_schema(schema_file)
            if not schema:
                return False
            
            # Create table
            table = bigquery.Table(table_ref, schema=schema)
            table.description = f"ustam App {table_name} data"
            
            # Set table labels
            table.labels = {
                "app": "ustam",
                "table_type": "analytics" if "analytics" in table_name or "logs" in table_name else "core",
                "created_by": "auto_setup"
            }
            
            # Configure partitioning for time-series tables
            if any(field.name == 'timestamp' for field in schema):
                table.time_partitioning = bigquery.TimePartitioning(
                    type_=bigquery.TimePartitioningType.DAY,
                    field='timestamp'
                )
            elif any(field.name == 'created_at' for field in schema):
                table.time_partitioning = bigquery.TimePartitioning(
                    type_=bigquery.TimePartitioningType.DAY,
                    field='created_at'
                )
            
            table = self.client.create_table(table)
            logger.info(f"âœ… Table {table_name} created successfully")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to create table {table_name}: {e}")
            return False

    def create_all_tables(self):
        """Create all tables defined in schema files"""
        logger.info("ğŸ“Š Creating BigQuery tables...")
        
        success_count = 0
        total_count = len(self.table_schemas)
        
        for table_name, schema_file in self.table_schemas.items():
            if self.create_table(table_name, schema_file):
                success_count += 1
            else:
                logger.error(f"âŒ Failed to create table: {table_name}")
        
        logger.info(f"ğŸ“Š Tables created: {success_count}/{total_count}")
        return success_count == total_count

    def create_view(self, view_name, query):
        """Create a BigQuery view"""
        try:
            view_ref = self.client.dataset(self.dataset_id).table(view_name)
            
            # Check if view exists
            try:
                self.client.get_table(view_ref)
                # Delete existing view to update it
                self.client.delete_table(view_ref)
                logger.info(f"ğŸ”„ Updating existing view {view_name}")
            except exceptions.NotFound:
                pass
            
            # Create view
            view = bigquery.Table(view_ref)
            view.view_query = query
            view.description = f"ustam App {view_name} analytics view"
            
            view = self.client.create_table(view)
            logger.info(f"âœ… View {view_name} created successfully")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to create view {view_name}: {e}")
            return False

    def create_all_views(self):
        """Create all analytics views"""
        logger.info("ğŸ“ˆ Creating analytics views...")
        
        success_count = 0
        total_count = len(self.analytics_views)
        
        for view_name, query in self.analytics_views.items():
            if self.create_view(view_name, query):
                success_count += 1
        
        logger.info(f"ğŸ“ˆ Views created: {success_count}/{total_count}")
        return success_count == total_count

    def setup_streaming_inserts(self):
        """Setup streaming insert configurations"""
        logger.info("ğŸ”„ Configuring streaming inserts...")
        
        # Tables that need streaming inserts
        streaming_tables = [
            'user_activity_logs',
            'error_logs', 
            'performance_metrics',
            'search_analytics',
            'payment_analytics'
        ]
        
        for table_name in streaming_tables:
            try:
                table_ref = self.client.dataset(self.dataset_id).table(table_name)
                table = self.client.get_table(table_ref)
                
                # Configure for streaming
                # Note: BigQuery automatically handles streaming inserts
                logger.info(f"âœ… Streaming configured for {table_name}")
                
            except Exception as e:
                logger.error(f"âŒ Failed to configure streaming for {table_name}: {e}")
        
        logger.info("ğŸ”„ Streaming insert configuration completed")

    def create_scheduled_queries(self):
        """Create scheduled queries for automated analytics"""
        logger.info("â° Setting up scheduled queries...")
        
        # Daily business metrics calculation
        daily_metrics_query = f"""
        INSERT INTO `{self.project_id}.{self.dataset_id}.business_metrics`
        SELECT
          GENERATE_UUID() as metric_id,
          CURRENT_DATE() as date,
          'daily_summary' as metric_type,
          (SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.users`) as total_users,
          (SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.users` WHERE is_active = true) as active_users,
          (SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.users` WHERE DATE(created_at) = CURRENT_DATE()) as new_users,
          (SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.craftsmen`) as total_craftsmen,
          (SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.craftsmen` WHERE is_available = true) as active_craftsmen,
          (SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.customers`) as total_customers,
          (SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.customers`) as active_customers,
          (SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.jobs`) as total_jobs,
          (SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.jobs` WHERE status = 'completed') as completed_jobs,
          (SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.jobs` WHERE status = 'pending') as pending_jobs,
          (SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.jobs` WHERE status = 'cancelled') as cancelled_jobs,
          COALESCE((SELECT SUM(amount) FROM `{self.project_id}.{self.dataset_id}.payments` WHERE status = 'completed'), 0) as total_revenue,
          COALESCE((SELECT SUM(platform_fee) FROM `{self.project_id}.{self.dataset_id}.payments` WHERE status = 'completed'), 0) as platform_fees,
          COALESCE((SELECT AVG(amount) FROM `{self.project_id}.{self.dataset_id}.payments` WHERE status = 'completed'), 0) as average_job_value,
          COALESCE((SELECT AVG(rating) FROM `{self.project_id}.{self.dataset_id}.reviews`), 0) as average_rating,
          (SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.messages`) as total_messages,
          (SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.quotes`) as total_quotes,
          COALESCE((SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.jobs`) / NULLIF((SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.quotes`), 0), 0) as conversion_rate,
          0.0 as user_retention_rate,  -- Will be calculated separately
          CURRENT_TIMESTAMP() as created_at
        """
        
        logger.info("â° Scheduled queries configuration ready")
        logger.info("Note: Use Google Cloud Console to create actual scheduled queries")
        logger.info(f"Query example saved for daily metrics calculation")

    def full_setup(self):
        """Run complete BigQuery setup"""
        logger.info("ğŸš€ ustam - COMPREHENSIVE BIGQUERY SETUP")
        logger.info("=" * 60)
        
        # Step 1: Check prerequisites
        if not self.check_prerequisites():
            logger.error("âŒ Prerequisites not met. Please fix and try again.")
            return False
        
        # Step 2: Initialize client
        if not self.initialize_client():
            logger.error("âŒ Failed to initialize BigQuery client")
            return False
        
        # Step 3: Create dataset
        if not self.create_dataset():
            logger.error("âŒ Failed to create dataset")
            return False
        
        # Step 4: Create all tables
        if not self.create_all_tables():
            logger.error("âŒ Failed to create all tables")
            return False
        
        # Step 5: Create analytics views
        if not self.create_all_views():
            logger.error("âŒ Failed to create all views")
            return False
        
        # Step 6: Setup streaming
        self.setup_streaming_inserts()
        
        # Step 7: Setup scheduled queries
        self.create_scheduled_queries()
        
        # Success message
        logger.info("âœ… COMPREHENSIVE SETUP COMPLETE!")
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š Project: {self.project_id}")
        logger.info(f"ğŸ“ˆ Dataset: {self.project_id}.{self.dataset_id}")
        logger.info(f"ğŸŒ Console: https://console.cloud.google.com/bigquery?project={self.project_id}")
        logger.info(f"ğŸ“Š Tables: {len(self.table_schemas)} created")
        logger.info(f"ğŸ“ˆ Views: {len(self.analytics_views)} created")
        logger.info("ğŸ”„ Streaming inserts configured")
        logger.info("â° Scheduled queries ready")
        
        return True

    def check_prerequisites(self):
        """Check if all prerequisites are met"""
        logger.info("ğŸ” Checking prerequisites...")
        
        # Check gcloud authentication with multiple possible paths
        gcloud_paths = [
            r'C:\Users\sudes\AppData\Local\Google\Cloud SDK\google-cloud-sdk\bin\gcloud.cmd',
            r'C:\Program Files (x86)\Google\Cloud SDK\google-cloud-sdk\bin\gcloud.cmd',
            'gcloud.cmd',
            'gcloud'
        ]
        
        gcloud_cmd = None
        for path in gcloud_paths:
            try:
                result = subprocess.run([path, 'auth', 'list'], capture_output=True, text=True)
                if result.returncode == 0:
                    gcloud_cmd = path
                    break
            except FileNotFoundError:
                continue
        
        if not gcloud_cmd:
            logger.error("âŒ gcloud command not found. Install Google Cloud SDK")
            return False
        
        # Check authentication
        try:
            result = subprocess.run([gcloud_cmd, 'auth', 'list'], capture_output=True, text=True)
            if 'ACTIVE' not in result.stdout:
                logger.error("âŒ No active gcloud authentication found. Run: gcloud auth login")
                return False
            logger.info("âœ… Google Cloud authentication verified")
        except Exception as e:
            logger.error(f"âŒ Authentication check failed: {e}")
            return False
        
        return True

    def create_dataset(self):
        """Create BigQuery dataset if it doesn't exist"""
        try:
            dataset_ref = self.client.dataset(self.dataset_id)
            
            try:
                dataset = self.client.get_dataset(dataset_ref)
                logger.info(f"âœ… Dataset {self.dataset_id} already exists")
            except exceptions.NotFound:
                # Create dataset
                dataset = bigquery.Dataset(dataset_ref)
                dataset.location = self.location
                dataset.description = "ustam App Comprehensive Analytics and Logging Data"
                
                # Set dataset labels
                dataset.labels = {
                    "app": "ustam",
                    "environment": "production",
                    "created_by": "comprehensive_setup",
                    "version": "2.0"
                }
                
                dataset = self.client.create_dataset(dataset)
                logger.info(f"âœ… Dataset {self.dataset_id} created successfully")
            
            return True
        except Exception as e:
            logger.error(f"âŒ Failed to create dataset: {e}")
            return False

    # Analytics view queries
    def _get_daily_user_stats_query(self):
        return f"""
        SELECT
          DATE(created_at) as date,
          user_type,
          COUNT(*) as new_users,
          SUM(COUNT(*)) OVER (PARTITION BY user_type ORDER BY DATE(created_at)) as cumulative_users
        FROM `{self.project_id}.{self.dataset_id}.users`
        GROUP BY DATE(created_at), user_type
        ORDER BY date DESC, user_type
        """

    def _get_craftsman_performance_query(self):
        return f"""
        SELECT
          c.user_id,
          u.first_name,
          u.last_name,
          c.business_name,
          c.average_rating,
          c.total_reviews,
          c.total_jobs,
          COALESCE(SUM(p.craftsman_amount), 0) as total_earnings,
          COUNT(DISTINCT j.id) as jobs_completed,
          AVG(DATETIME_DIFF(j.completed_at, j.created_at, HOUR)) as avg_completion_time_hours
        FROM `{self.project_id}.{self.dataset_id}.craftsmen` c
        JOIN `{self.project_id}.{self.dataset_id}.users` u ON c.user_id = u.user_id
        LEFT JOIN `{self.project_id}.{self.dataset_id}.jobs` j ON j.assigned_craftsman_id = c.id
        LEFT JOIN `{self.project_id}.{self.dataset_id}.payments` p ON p.job_id = j.id AND p.status = 'completed'
        GROUP BY c.user_id, u.first_name, u.last_name, c.business_name, c.average_rating, c.total_reviews, c.total_jobs
        ORDER BY total_earnings DESC
        """

    def _get_revenue_dashboard_query(self):
        return f"""
        SELECT
          DATE(created_at) as date,
          payment_type,
          COUNT(*) as transaction_count,
          SUM(amount) as total_amount,
          SUM(platform_fee) as total_platform_fees,
          AVG(amount) as avg_transaction_value,
          SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as successful_payments,
          SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failed_payments
        FROM `{self.project_id}.{self.dataset_id}.payments`
        GROUP BY DATE(created_at), payment_type
        ORDER BY date DESC, payment_type
        """

    def _get_search_insights_query(self):
        return f"""
        SELECT
          DATE(timestamp) as date,
          search_type,
          COUNT(*) as total_searches,
          AVG(results_count) as avg_results_count,
          AVG(response_time_ms) as avg_response_time_ms,
          SUM(CASE WHEN clicked_result_id IS NOT NULL THEN 1 ELSE 0 END) as searches_with_clicks,
          SUM(CASE WHEN clicked_result_id IS NOT NULL THEN 1 ELSE 0 END) / COUNT(*) as click_through_rate
        FROM `{self.project_id}.{self.dataset_id}.search_analytics`
        GROUP BY DATE(timestamp), search_type
        ORDER BY date DESC, search_type
        """

    def _get_error_summary_query(self):
        return f"""
        SELECT
          DATE(timestamp) as date,
          error_type,
          error_level,
          COUNT(*) as error_count,
          COUNT(DISTINCT user_id) as affected_users,
          COUNT(DISTINCT session_id) as affected_sessions,
          SUM(CASE WHEN resolved = true THEN 1 ELSE 0 END) as resolved_errors
        FROM `{self.project_id}.{self.dataset_id}.error_logs`
        GROUP BY DATE(timestamp), error_type, error_level
        ORDER BY date DESC, error_count DESC
        """

    def _get_platform_comparison_query(self):
        return f"""
        SELECT
          platform,
          COUNT(DISTINCT user_id) as unique_users,
          COUNT(*) as total_actions,
          AVG(duration_ms) as avg_action_duration,
          SUM(CASE WHEN success = true THEN 1 ELSE 0 END) / COUNT(*) as success_rate
        FROM `{self.project_id}.{self.dataset_id}.user_activity_logs`
        WHERE DATE(timestamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
        GROUP BY platform
        ORDER BY unique_users DESC
        """

    def _get_business_kpis_query(self):
        return f"""
        SELECT
          'Current' as period,
          (SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.users` WHERE is_active = true) as active_users,
          (SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.craftsmen` WHERE is_available = true) as available_craftsmen,
          (SELECT COUNT(*) FROM `{self.project_id}.{self.dataset_id}.jobs` WHERE status = 'pending') as pending_jobs,
          (SELECT COALESCE(SUM(amount), 0) FROM `{self.project_id}.{self.dataset_id}.payments` WHERE status = 'completed' AND DATE(created_at) = CURRENT_DATE()) as today_revenue,
          (SELECT COALESCE(SUM(amount), 0) FROM `{self.project_id}.{self.dataset_id}.payments` WHERE status = 'completed' AND DATE(created_at) >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)) as monthly_revenue,
          (SELECT COALESCE(AVG(rating), 0) FROM `{self.project_id}.{self.dataset_id}.reviews` WHERE DATE(created_at) >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)) as monthly_avg_rating,
          CURRENT_TIMESTAMP() as calculated_at
        """

def main():
    """Main function"""
    project_id = sys.argv[1] if len(sys.argv) > 1 else "ustam-analytics"
    
    print(f"""
ğŸš€ ustam - COMPREHENSIVE BIGQUERY SETUP
==================================================
ğŸ“Š Project ID: {project_id}
ğŸ“ˆ Dataset: ustam_analytics
ğŸŒ Location: US
â° Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
==================================================
    """)
    
    # Initialize uploader
    uploader = ComprehensiveBigQuerySetup(project_id)
    
    # Run full setup
    success = uploader.full_setup()
    
    if success:
        print(f"""
ğŸ‰ SETUP SUCCESSFUL!
==================================================
âœ… All tables and views created
âœ… Analytics infrastructure ready
âœ… Streaming inserts configured
âœ… Ready for real-time data ingestion

ğŸ”— Next Steps:
1. Run data upload: python bigquery_auto_upload.py {project_id}
2. Setup real-time logging in your app
3. Create BigQuery dashboards
4. Setup scheduled queries in Google Cloud Console

ğŸ“Š BigQuery Console: https://console.cloud.google.com/bigquery?project={project_id}
        """)
    else:
        print("""
âŒ SETUP FAILED!
==================================================
Please check the error messages above and fix any issues.
        """)
    
    return success

if __name__ == '__main__':
    main()